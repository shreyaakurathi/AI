{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SHREYA AKURATHI**\n",
        "**200968188**\n",
        "**WEEK 5**"
      ],
      "metadata": {
        "id": "okn622SvJSO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tf-agents"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: tf-agents in /usr/local/lib/python3.8/dist-packages (0.15.0)\nRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.2.1)\nRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.22.4)\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\nRequirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (3.19.6)\nRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (4.5.0)\nRequirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.19.0)\nRequirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.5.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents) (8.4.0)\nRequirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.1.0)\nRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.4.0)\nRequirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.23.0)\nRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\nRequirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (6.0.0)\nRequirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (4.4.2)\nRequirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.8)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.15.0)\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlx8p06nEJxz",
        "outputId": "ec21b87e-97f9-41db-d31c-522ac5a1571e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "from tf_agents.utils import common\n",
        "import numpy as np\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "XOg4WBqfIGKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1 -Create a environment  \n",
        "\n",
        "1.  for  which  the  observation  is  a  random  integer  between -5and 5,  there  are  3 possible actions (0, 1, 2), and the reward is the product of the action and the observation.\n",
        "2. Define anoptimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive.\n",
        "3. Request  for 50observations  from  the  environment, compute  and  print the total reward."
      ],
      "metadata": {
        "id": "mO2vSk-KH7zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BanditPyEnvironment(py_environment.PyEnvironment):\n",
        "    def __init__(self):\n",
        "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=int, minimum=0, maximum=2, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(shape=(1,), dtype=int, minimum=-5, maximum=5, name='observation')\n",
        "        self._state = None\n",
        "        self._episode_ended = False\n",
        "        self._observation = None\n",
        "        self._reward = None\n",
        "    \n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "    \n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "    \n",
        "    def _reset(self):\n",
        "        self._episode_ended = False\n",
        "        self._observation = np.random.randint(low=-5, high=6)\n",
        "        self._reward = 0\n",
        "        return ts.restart(np.array(self._observation, dtype=np.int32))\n",
        "    \n",
        "    def _step(self, action):\n",
        "      if self._episode_ended:\n",
        "        return self.reset()\n",
        "\n",
        "      self._reward = self._observation * action\n",
        "      self._episode_ended = True\n",
        "      return ts.termination(np.array(self._observation, dtype=np.int32), reward=self._reward)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "fsyGfFySH7il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = BanditPyEnvironment()"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "8pf3NpLyHjbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_step = env.reset()\n",
        "print(time_step)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TimeStep(\n{'discount': array(1., dtype=float32),\n 'observation': array(-5, dtype=int32),\n 'reward': array(0., dtype=float32),\n 'step_type': array(0, dtype=int32)})\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4OO-bkRIXUm",
        "outputId": "f5c5773f-7394-4e4b-853c-cd731be3da24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action = tf.constant(1, dtype=tf.int32)\n",
        "next_time_step = env.step(action)\n",
        "print(next_time_step)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TimeStep(\n{'discount': array(0., dtype=float32),\n 'observation': array(-5, dtype=int32),\n 'reward': array(-5., dtype=float32),\n 'step_type': array(2, dtype=int32)})\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPEN7EtGIZQf",
        "outputId": "d8d9a6a7-fdec-44eb-98c0-cbbbdbe535d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimal_policy(observation):\n",
        "    if observation < 0:\n",
        "        return tf.constant(0, dtype=tf.int32)\n",
        "    else:\n",
        "        return tf.constant(2, dtype=tf.int32)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "KC-371gvIcM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_reward = 0.0 \n",
        "time_step = env.reset()\n",
        "for _ in range(50):\n",
        "    action = optimal_policy(time_step.observation)\n",
        "    time_step = env.step(action)\n",
        "    total_reward += time_step.reward\n",
        "\n",
        "print('Total reward:', total_reward)  \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total reward: 76.0\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1H3FgGbIeRT",
        "outputId": "87fc3228-b8fc-4ccc-b3b7-ebdc856be915"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Exercise 2 â€“Create an environment \n",
        "\n",
        "\n",
        "1. Define an environment will either always give reward = observation * action or reward = -observation * action. This will be decided when the environment is initialized.\n",
        "\n",
        "2. Define a policy that detects the behavior of the underlying environment. There are three situations that the policy needs to handle:\n",
        "\n",
        "> i.The agent has not detected know yet which version of the environment is running.ii.The  agent  detected  that  the  original  version  of  the  environment  is running.iii.The  agent  detected  that  the  flipped  version  of  the  environment  is running\n",
        "\n",
        "3. Define the agent that detects the sign of the environment and sets the policy appropriately\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aKIFMJU-LREt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomEnvironment(py_environment.PyEnvironment):\n",
        "    def __init__(self, reward_sign):\n",
        "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=int, minimum=0, maximum=2, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(shape=(1,), dtype=int, minimum=-5, maximum=5, name='observation')\n",
        "        self._state = None\n",
        "        self._episode_ended = False\n",
        "        self._observation = None\n",
        "        self._reward_sign = reward_sign\n",
        "\n",
        "    \n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "    \n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "    \n",
        "    def _reset(self):\n",
        "      self._episode_ended = False\n",
        "      self._observation = np.random.randint(low=-5, high=6)\n",
        "      self._reward = 0\n",
        "      return ts.restart(np.array(self._observation, dtype=np.int32))\n",
        "    \n",
        "    def _step(self, action):\n",
        "        if self._episode_ended:\n",
        "            return self.reset()\n",
        "        if self._reward_sign == 'original':\n",
        "          self._reward = self._observation * action\n",
        "        else:\n",
        "          self._reward = -self._observation * action\n",
        "\n",
        "        self._episode_ended = True\n",
        "        return ts.termination(np.array(self._observation, dtype=np.int32), reward=self._reward)\n",
        "\n",
        "        \n"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "id": "0DUMWHq_KZuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy:\n",
        "  def __init__(self):\n",
        "    self._state = 'unknown'\n",
        "\n",
        "  def get_action(self, observation):\n",
        "    if self._state == 'unknown':\n",
        "      if observation >= 0:\n",
        "        self._state = 'original'\n",
        "        return 2\n",
        "      else:\n",
        "        self._state = 'flipped'\n",
        "        return 0\n",
        "    elif self._state == 'original':\n",
        "      return 2\n",
        "    else:\n",
        "      return 0"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "id": "4hfjWoBVM2Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self):\n",
        "    self._policy = Policy()\n",
        "\n",
        "  def update_policy(self, reward_sign):\n",
        "    if reward_sign == 'original':\n",
        "      self._policy._state = 'original'\n",
        "    else:\n",
        "      self._policy._state = 'flipped'\n",
        "\n",
        "  def get_action(self, observation):\n",
        "    return self._policy.get_action(observation)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "id": "zoTtdLtdM5Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_env = CustomEnvironment(reward_sign='original')\n",
        "agent = Agent()\n",
        "\n",
        "total_reward = 0\n",
        "for i in range(50):\n",
        "    observation = reward_env.reset().observation\n",
        "    agent.update_policy('original')\n",
        "    action = agent.get_action(observation)\n",
        "    time_step = reward_env.step(action)\n",
        "    total_reward += time_step.reward\n",
        "print(\"Total reward: \", total_reward)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total reward:  4.0\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEOc9dtuM70M",
        "outputId": "e23818be-040b-4d85-d1f2-a0889d72f9fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total reward should be positive since the optimal policy will result in positive rewards for positive observations and negative rewards for negative observations."
      ],
      "metadata": {
        "id": "XPgEIzYDod7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_env = CustomEnvironment(reward_sign='flipped')\n",
        "agent = Agent()\n",
        "\n",
        "total_reward = 0\n",
        "for i in range(50):\n",
        "    observation = reward_env.reset().observation\n",
        "    agent.update_policy('flipped')\n",
        "    action = agent.get_action(observation)\n",
        "    time_step = reward_env.step(action)\n",
        "    total_reward += time_step.reward\n",
        "print(\"Total reward: \", total_reward)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total reward:  0.0\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6WcBlHqTXWs",
        "outputId": "4d79a8eb-49ca-4152-f6eb-02aa2dda2f73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This defines an environment where the reward is always 0, regardless of the action and observation. Therefore, the total reward in this environment should always be 0.\n"
      ],
      "metadata": {
        "id": "cneiikLxoe_O"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}