{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-agents[reverb]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting tf-agents[reverb]\n  Downloading tf_agents-0.15.0-py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (1.15.0)\nRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (2.2.1)\nCollecting pygame==2.1.0\n  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (0.5.0)\nRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (1.14.1)\nRequirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (3.19.6)\nCollecting gym<=0.23.0,>=0.17.0\n  Downloading gym-0.23.0.tar.gz (624 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 KB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (7.1.2)\nCollecting tensorflow-probability>=0.18.0\n  Downloading tensorflow_probability-0.19.0-py2.py3-none-any.whl (6.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (4.4.0)\nRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (1.4.0)\nRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (1.21.6)\nCollecting rlds\n  Downloading rlds-0.1.7-py3-none-manylinux2010_x86_64.whl (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tensorflow==2.11.0\n  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting dm-reverb~=0.10.0\n  Downloading dm_reverb-0.10.0-cp38-cp38-manylinux2014_x86_64.whl (6.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (57.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (1.51.1)\nCollecting keras<2.12,>=2.11.0\n  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (3.3.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (0.30.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (1.6.3)\nRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (3.1.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (2.2.0)\nCollecting tensorboard<2.12,>=2.11\n  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (15.0.6.1)\nCollecting flatbuffers>=2.0\n  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (23.0)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (0.4.0)\nRequirement already satisfied: portpicker in /usr/local/lib/python3.8/dist-packages (from dm-reverb~=0.10.0->tf-agents[reverb]) (1.3.9)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from dm-reverb~=0.10.0->tf-agents[reverb]) (0.1.8)\nRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\nRequirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (6.0.0)\nRequirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents[reverb]) (4.4.2)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0->tf-agents[reverb]) (0.38.4)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (3.12.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (3.4.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (2.16.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (1.8.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (1.0.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (2.25.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (0.6.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (0.4.6)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (0.2.8)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (5.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (1.3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (2022.12.7)\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (1.24.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (3.2.2)\nBuilding wheels for collected packages: gym\n  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697661 sha256=1ddb8a7150ab0da74773cec0667e92e79b7f8054a8cd94a05525501f577cb746\n  Stored in directory: /root/.cache/pip/wheels/e7/2f/ab/68bf956c5dde73c1856d981e54292cf58385fb60bca10b7acd\nSuccessfully built gym\nInstalling collected packages: flatbuffers, tensorflow-probability, tensorflow-estimator, rlds, pygame, keras, dm-reverb, gym, tf-agents, tensorboard, tensorflow\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 1.12\n    Uninstalling flatbuffers-1.12:\n      Successfully uninstalled flatbuffers-1.12\n  Attempting uninstall: tensorflow-probability\n    Found existing installation: tensorflow-probability 0.17.0\n    Uninstalling tensorflow-probability-0.17.0:\n      Successfully uninstalled tensorflow-probability-0.17.0\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.9.0\n    Uninstalling tensorflow-estimator-2.9.0:\n      Successfully uninstalled tensorflow-estimator-2.9.0\n  Attempting uninstall: keras\n    Found existing installation: keras 2.9.0\n    Uninstalling keras-2.9.0:\n      Successfully uninstalled keras-2.9.0\n  Attempting uninstall: gym\n    Found existing installation: gym 0.25.2\n    Uninstalling gym-0.25.2:\n      Successfully uninstalled gym-0.25.2\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.9.1\n    Uninstalling tensorboard-2.9.1:\n      Successfully uninstalled tensorboard-2.9.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.9.2\n    Uninstalling tensorflow-2.9.2:\n      Successfully uninstalled tensorflow-2.9.2\nSuccessfully installed dm-reverb-0.10.0 flatbuffers-23.1.21 gym-0.23.0 keras-2.11.0 pygame-2.1.0 rlds-0.1.7 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-probability-0.19.0 tf-agents-0.15.0\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRiIxdlet7Wz",
        "outputId": "11bf653f-095e-49de-83dd-73a5330f4db3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "6sewViA_INie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TFEnvironment(object):\n",
        "\n",
        "  def time_step_spec(self):\n",
        "    \"\"\"Describes the `TimeStep` tensors returned by `step()`.\"\"\"\n",
        "\n",
        "  def observation_spec(self):\n",
        "    \"\"\"Defines the `TensorSpec` of observations provided by the environment.\"\"\"\n",
        "\n",
        "  def action_spec(self):\n",
        "    \"\"\"Describes the TensorSpecs of the action expected by `step(action)`.\"\"\"\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
        "    return self._reset()\n",
        "\n",
        "  def current_time_step(self):\n",
        "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
        "    return self._current_time_step()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\"\n",
        "    return self._step(action)\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _reset(self):\n",
        "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _current_time_step(self):\n",
        "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _step(self, action):\n",
        "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "GqtfmDPhIRmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
        "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
        "print(\"Action Specs:\", tf_env.action_spec())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "True\nTimeStep Specs: TimeStep(\n{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n      dtype=float32)),\n 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\nAction Specs: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBAZBdFKIUuG",
        "outputId": "880bbdec-6219-4889-f8d8-98a79efbae92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "# reset() creates the initial time_step after resetting the environment.\n",
        "time_step = tf_env.reset()\n",
        "num_steps = 10\n",
        "transitions = []\n",
        "reward = 0\n",
        "for i in range(num_steps):\n",
        "  action = tf.constant([i % 2])\n",
        "  # applies the action and returns the new TimeStep.\n",
        "  next_time_step = tf_env.step(action)\n",
        "  transitions.append([time_step, action, next_time_step])\n",
        "  reward += next_time_step.reward\n",
        "  time_step = next_time_step\n",
        "\n",
        "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
        "print('\\n'.join(map(str, np_transitions)))\n",
        "print('Total reward:', reward.numpy())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[0.04599188, 0.00850259, 0.00686444, 0.00495125]], dtype=float32),\n 'reward': array([0.], dtype=float32),\n 'step_type': array([0], dtype=int32)}), array([0], dtype=int32), TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[ 0.04616193, -0.18671714,  0.00696346,  0.29979205]],\n      dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)})]\n[TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[ 0.04616193, -0.18671714,  0.00696346,  0.29979205]],\n      dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[0.04242759, 0.00830487, 0.01295931, 0.00931339]], dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)})]\n[TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[0.04242759, 0.00830487, 0.01295931, 0.00931339]], dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[ 0.04259369, -0.18700051,  0.01314557,  0.3060568 ]],\n      dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)})]\n[TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[ 0.04259369, -0.18700051,  0.01314557,  0.3060568 ]],\n      dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[0.03885368, 0.00793167, 0.01926671, 0.01754847]], dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)})]\n[TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[0.03885368, 0.00793167, 0.01926671, 0.01754847]], dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[ 0.03901231, -0.18746121,  0.01961768,  0.31624746]],\n      dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)})]\n[TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[ 0.03901231, -0.18746121,  0.01961768,  0.31624746]],\n      dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[0.03526309, 0.00737589, 0.02594263, 0.02981527]], dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)})]\n[TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[0.03526309, 0.00737589, 0.02594263, 0.02981527]], dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[ 0.03541061, -0.1881083 ,  0.02653893,  0.33056918]],\n      dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)})]\n[TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[ 0.03541061, -0.1881083 ,  0.02653893,  0.33056918]],\n      dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[0.03164844, 0.00662602, 0.03315032, 0.04637208]], dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)})]\n[TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[0.03164844, 0.00662602, 0.03315032, 0.04637208]], dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[ 0.03178096, -0.18895522,  0.03407776,  0.34932715]],\n      dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)})]\n[TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[ 0.03178096, -0.18895522,  0.03407776,  0.34932715]],\n      dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n{'discount': array([1.], dtype=float32),\n 'observation': array([[0.02800186, 0.00566591, 0.0410643 , 0.06758188]], dtype=float32),\n 'reward': array([1.], dtype=float32),\n 'step_type': array([1], dtype=int32)})]\nTotal reward: [10.]\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4w2YKHDIayO",
        "outputId": "f4b1de6e-241a-4816-fd48-d881e1768989"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "time_step = tf_env.reset()\n",
        "rewards = []\n",
        "steps = []\n",
        "num_episodes = 65\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "  episode_reward = 0\n",
        "  episode_steps = 0\n",
        "  while not time_step.is_last():\n",
        "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
        "    time_step = tf_env.step(action)\n",
        "    episode_steps += 1\n",
        "    episode_reward += time_step.reward.numpy()\n",
        "  rewards.append(episode_reward)\n",
        "  steps.append(episode_steps)\n",
        "  time_step = tf_env.reset()\n",
        "\n",
        "num_steps = np.sum(steps)\n",
        "avg_length = np.mean(steps)\n",
        "avg_reward = np.mean(rewards)\n",
        "\n",
        "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
        "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "num_episodes: 65 num_steps: 1513\navg_length 23.276923076923076 avg_reward: 23.276922\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHd35bzXI8Mm",
        "outputId": "4606fce6-b910-4259-86a5-fc284779afbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rewards earned in both approaches are different because in the steps approach, the environment is run for a fixed number of steps, while in the episode approach, the environment is run for a fixed number of episodes. \n",
        "In the first approach, the agent may not complete an episode within the given number of steps, while in the second approach, the agent is guaranteed to complete a certain number of episodes.\n",
        "\n",
        "Since the reward in the CartPole environment is 1 for every step the pole remains upright, the reward in the first approach will be lower compared to the second approach as the agent may not complete an episode within the given number of steps. \n",
        "The average reward in the second approach will be higher because it is an average over a certain number of completed episodes.\n",
        "\n",
        "As we can notice, when run for 10 steps the reward is 10. \n",
        "The max reward is 1 for each step and the episodes end if the cart has crossed a distance of 2.5 or the angle of the pole is greater than degrees or the episode length is greater than 200 steps.\n",
        "\n",
        "Since there is no actual learning being done here the episode lengths are very short compared to the 200 limit.\n",
        "Running it for a certain amount of steps without any learning would also end quickly as teh average episode length was 10.\n",
        "There seems to be a discount on the reward earned in both approaches so the reward is always 1 for a succesful action\n"
      ],
      "metadata": {
        "id": "cnsxrrnDoUeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cf3Hqt9MqsJu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python",
      "language": "python",
      "display_name": "Pyolite (preview)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "kernel_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}